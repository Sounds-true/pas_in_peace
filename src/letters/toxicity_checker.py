"""Toxicity analysis for letters using Detoxify + LLM."""

from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import re

try:
    from detoxify import Detoxify
    DETOXIFY_AVAILABLE = True
except ImportError:
    DETOXIFY_AVAILABLE = False

try:
    from langchain_openai import ChatOpenAI
    from langchain.schema import HumanMessage, SystemMessage
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False

from src.core.logger import get_logger
from src.core.config import settings


logger = get_logger(__name__)


@dataclass
class ToxicPhrase:
    """A toxic phrase detected in text."""
    text: str
    type: str  # insult, threat, obscene, etc.
    score: float  # 0.0-1.0
    context: str  # Surrounding text
    suggestion: Optional[str] = None  # LLM suggestion for replacement


@dataclass
class ToxicityAnalysis:
    """Complete toxicity analysis result."""
    overall_score: float  # 0.0-1.0 combined toxicity
    is_toxic: bool  # Above threshold?
    scores: Dict[str, float]  # Detoxify scores
    toxic_phrases: List[ToxicPhrase]
    llm_recommendation: Optional[str] = None  # Detailed LLM analysis
    safe_alternative: Optional[str] = None  # LLM rewritten version


class ToxicityChecker:
    """
    Check letter toxicity using Detoxify + LLM.

    Flow:
    1. Detoxify detects toxic patterns
    2. Extract specific toxic phrases
    3. LLM provides detailed recommendations
    """

    def __init__(self):
        """Initialize toxicity checker."""
        self.detoxify = None
        self.llm = None
        self.initialized = False

        # Toxic patterns for Russian text
        self.toxic_patterns = {
            'insult': [
                r'\b—Å—É–∫–∞\b', r'\b—Å–≤–æ–ª–æ—á—å\b', r'\b—É–±–ª[—é—è]–¥–æ–∫\b', r'\b–∏–¥[–∏–µ]–æ—Ç\b',
                r'\b–¥—É—Ä[–∞–æ]–∫\b', r'\b–¥[–µ–∏]–±–∏–ª–∞\w*', r'\b—É—Ä–æ–¥\w*', r'\b—Ç–≤–∞—Ä—å\b'
            ],
            'threat': [
                r'–∑–∞–ø–ª–∞—Ç[–∏—å]\w*', r'–ø–æ–∂–∞–ª–µ[–µ—é–π—è]\w*', r'–æ—Ç–æ–º—â\w*', r'–Ω–∞–∫–∞–∂—É',
                r'–ø–æ–¥–∞–º\s+–≤\s+—Å—É–¥', r'–ª–∏—à[—É—é–∏—å]\w*\s+—Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö'
            ],
            'blame': [
                r'—Ç—ã\s+–≤–∏–Ω–æ–≤–∞—Ç\w*', r'–∏–∑-–∑–∞\s+—Ç–µ–±—è', r'—Ç–≤–æ—è\s+–≤–∏–Ω–∞',
                r'—Ç—ã\s+—Ä–∞–∑—Ä—É—à[–∏–µ]\w*', r'—Ç—ã\s+—É–∫—Ä–∞[–ª–¥]\w*'
            ],
            'manipulation': [
                r'—Ä–µ–±—ë–Ω–æ–∫\s+–Ω–µ\s+—Ö–æ—á–µ—Ç', r'—Ä–µ–±—ë–Ω–æ–∫\s+–±–æ–∏—Ç—Å—è',
                r'–≤–∏–¥–∏—à—å\s+—á—Ç–æ\s+—Ç—ã\s+—Å–¥–µ–ª–∞[–ª–¥]', r'–¥–µ—Ç–∏\s+—Å—Ç—Ä–∞–¥–∞—é—Ç\s+–∏–∑-–∑–∞\s+—Ç–µ–±—è'
            ]
        }

    async def initialize(self) -> bool:
        """Initialize Detoxify and LLM."""
        if self.initialized:
            return True

        # Initialize Detoxify
        if DETOXIFY_AVAILABLE:
            try:
                self.detoxify = Detoxify('multilingual')
                logger.info("detoxify_initialized")
            except Exception as e:
                logger.warning("detoxify_init_failed", error=str(e))
                return False
        else:
            logger.warning("detoxify_not_available")
            return False

        # Initialize LLM (optional)
        if LLM_AVAILABLE and hasattr(settings, 'OPENAI_API_KEY'):
            try:
                self.llm = ChatOpenAI(
                    model="gpt-4",
                    temperature=0.3,
                    api_key=settings.OPENAI_API_KEY
                )
                logger.info("llm_initialized_for_toxicity")
            except Exception as e:
                logger.warning("llm_init_failed", error=str(e))
                self.llm = None

        self.initialized = True
        return True

    async def analyze(
        self,
        text: str,
        threshold: float = 0.5,
        use_llm: bool = True
    ) -> ToxicityAnalysis:
        """
        Analyze text toxicity.

        Args:
            text: Text to analyze
            threshold: Toxicity threshold (0.0-1.0)
            use_llm: Use LLM for detailed recommendations

        Returns:
            ToxicityAnalysis with scores and recommendations
        """
        if not self.initialized:
            await self.initialize()

        # Step 1: Detoxify analysis
        detoxify_scores = self._detoxify_analyze(text)

        # Step 2: Extract toxic phrases
        toxic_phrases = self._extract_toxic_phrases(text, detoxify_scores)

        # Step 3: Calculate overall score
        overall_score = max(detoxify_scores.values()) if detoxify_scores else 0.0
        is_toxic = overall_score >= threshold

        # Step 4: LLM recommendations (if toxic and LLM available)
        llm_recommendation = None
        safe_alternative = None

        if is_toxic and use_llm and self.llm:
            try:
                llm_result = await self._get_llm_recommendations(text, toxic_phrases)
                llm_recommendation = llm_result.get('recommendation')
                safe_alternative = llm_result.get('alternative')
            except Exception as e:
                logger.error("llm_recommendation_failed", error=str(e))

        return ToxicityAnalysis(
            overall_score=overall_score,
            is_toxic=is_toxic,
            scores=detoxify_scores,
            toxic_phrases=toxic_phrases,
            llm_recommendation=llm_recommendation,
            safe_alternative=safe_alternative
        )

    def _detoxify_analyze(self, text: str) -> Dict[str, float]:
        """Run Detoxify analysis."""
        if not self.detoxify:
            return {}

        try:
            results = self.detoxify.predict(text)
            return {
                'toxicity': float(results.get('toxicity', 0)),
                'severe_toxicity': float(results.get('severe_toxicity', 0)),
                'obscene': float(results.get('obscene', 0)),
                'threat': float(results.get('threat', 0)),
                'insult': float(results.get('insult', 0)),
                'identity_attack': float(results.get('identity_attack', 0))
            }
        except Exception as e:
            logger.error("detoxify_analysis_failed", error=str(e))
            return {}

    def _extract_toxic_phrases(
        self,
        text: str,
        scores: Dict[str, float]
    ) -> List[ToxicPhrase]:
        """Extract specific toxic phrases using patterns."""
        toxic_phrases = []
        text_lower = text.lower()

        # Check each pattern category
        for category, patterns in self.toxic_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text_lower, re.IGNORECASE)
                for match in matches:
                    phrase = text[match.start():match.end()]

                    # Get context (20 chars before/after)
                    start = max(0, match.start() - 20)
                    end = min(len(text), match.end() + 20)
                    context = text[start:end]

                    toxic_phrases.append(ToxicPhrase(
                        text=phrase,
                        type=category,
                        score=scores.get(category, scores.get('toxicity', 0.5)),
                        context=context
                    ))

        return toxic_phrases

    async def _get_llm_recommendations(
        self,
        text: str,
        toxic_phrases: List[ToxicPhrase]
    ) -> Dict[str, str]:
        """Get detailed recommendations from LLM."""
        if not self.llm:
            return {}

        # Build toxic phrases summary
        phrases_summary = "\n".join([
            f"- '{phrase.text}' (—Ç–∏–ø: {phrase.type}, –∫–æ–Ω—Ç–µ–∫—Å—Ç: ...{phrase.context}...)"
            for phrase in toxic_phrases[:5]  # Limit to top 5
        ])

        system_prompt = """–¢—ã - –ø—Å–∏—Ö–æ–ª–æ–≥, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º –æ—Ç—á—É–∂–¥–µ–Ω–∏–∏.
–ü–æ–º–æ–≥–∞–µ—à—å —Ä–æ–¥–∏—Ç–µ–ª—è–º –ø–∏—Å–∞—Ç—å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ –ø–∏—Å—å–º–∞.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –û–±—ä—è—Å–Ω–∏—Ç—å –ø–æ—á–µ–º—É —Ç–æ–∫—Å–∏—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã –≤—Ä–µ–¥–Ω—ã (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è —Ä–µ–±—ë–Ω–∫–∞ –≤ –±—É–¥—É—â–µ–º)
2. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
3. –ë—ã—Ç—å —ç–º–ø–∞—Ç–∏—á–Ω—ã–º –Ω–æ —á–µ—Å—Ç–Ω—ã–º"""

        user_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø–∏—Å—å–º–æ –Ω–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å:

–¢–ï–ö–°–¢:
{text}

–û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:
{phrases_summary if phrases_summary else "–û–±—â–∞—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑"}

–î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
1. –ü–æ—á–µ–º—É —ç—Ç–∏ —Ñ—Ä–∞–∑—ã –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω—ã? (–æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ —Ä–µ–±—ë–Ω–æ–∫ –ø—Ä–æ—á–∏—Ç–∞–µ—Ç –≤ –±—É–¥—É—â–µ–º)
2. –ö–∞–∫ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ?
3. –ü—Ä–µ–¥–ª–æ–∂–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –≤–µ—Ä—Å–∏—é –ø–∏—Å—å–º–∞.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
–ü–û–ß–ï–ú–£ –ü–†–û–ë–õ–ï–ú–ê–¢–ò–ß–ù–û:
[–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ]

–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
[–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ–≤–µ—Ç—ã]

–ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê:
[–ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∏—Å—å–º–∞]"""

        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]

            response = await self.llm.ainvoke(messages)
            content = response.content

            # Parse response
            recommendation = content
            alternative = None

            if "–ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê:" in content:
                parts = content.split("–ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê:")
                recommendation = parts[0].strip()
                alternative = parts[1].strip() if len(parts) > 1 else None

            return {
                'recommendation': recommendation,
                'alternative': alternative
            }

        except Exception as e:
            logger.error("llm_analysis_failed", error=str(e))
            return {}

    def format_warnings(self, analysis: ToxicityAnalysis) -> str:
        """Format toxicity warnings for user."""
        if not analysis.is_toxic:
            return "‚úÖ –ü–∏—Å—å–º–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Ñ—Ä–∞–∑."

        warnings = []
        warnings.append(f"‚ö†Ô∏è **–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å: {analysis.overall_score:.0%}**\n")

        # List toxic phrases
        if analysis.toxic_phrases:
            warnings.append("**–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ñ—Ä–∞–∑—ã:**")
            for phrase in analysis.toxic_phrases[:5]:
                type_emoji = {
                    'insult': 'üò†',
                    'threat': '‚ö°',
                    'blame': 'üëâ',
                    'manipulation': 'üé≠'
                }.get(phrase.type, '‚ö†Ô∏è')

                warnings.append(
                    f"{type_emoji} \"{phrase.text}\" (—Ç–∏–ø: {phrase.type})"
                )

        # Add LLM recommendation
        if analysis.llm_recommendation:
            warnings.append(f"\nüí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n{analysis.llm_recommendation}")

        # Add safe alternative
        if analysis.safe_alternative:
            warnings.append(f"\n‚úçÔ∏è **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç:**\n{analysis.safe_alternative}")

        return "\n".join(warnings)
